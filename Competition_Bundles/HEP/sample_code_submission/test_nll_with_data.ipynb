{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_valid_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import path \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "current_path = os.getcwd()\n",
    "\n",
    "systematics_path = os.path.join(current_path,'ingestion', 'systematics')\n",
    "path.append(systematics_path)\n",
    "from systematics import postprocess \n",
    "input_dir = os.path.join(\"D:\",\"Higgs_Uncertainity_challenge_input_data\")\n",
    "train_data_file = os.path.join(input_dir, 'train', 'data', 'data.csv')\n",
    "train_labels_file = os.path.join(input_dir, 'train', 'labels', \"data.labels\")\n",
    "train_settings_file = os.path.join(input_dir, 'train', 'settings', \"data.json\")\n",
    "train_weights_file = os.path.join(input_dir, 'train', 'weights', \"data.weights\")\n",
    "\n",
    "# read train data\n",
    "data = pd.read_csv(train_data_file)\n",
    "\n",
    "# read train labels\n",
    "with open(train_labels_file, \"r\") as f:\n",
    "    labels = np.array(f.read().splitlines(), dtype=float)\n",
    "\n",
    "# read train settings\n",
    "with open(train_settings_file) as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "# read train weights\n",
    "with open(train_weights_file) as f:\n",
    "    weights = np.array(f.read().splitlines(), dtype=float\n",
    "                       )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.5\n",
    "gamma_roi = train_weights[train_labels == 1].sum()\n",
    "beta_roi = train_weights[train_labels == 0].sum()\n",
    "\n",
    "def _sigma_asimov_CR(mu):\n",
    "    return 0\n",
    "\n",
    "def calculate_NLL( mu_scan, weight_data,use_CR=False):\n",
    "\n",
    "    def _sigma_asimov_SR(mu):\n",
    "        return mu*gamma_roi + beta_roi\n",
    "\n",
    "    sum_data_total_SR = weight_data.sum()\n",
    "    sum_data_total_CR = 0\n",
    "    comb_llr = []\n",
    "    for i, mu in enumerate(mu_scan):\n",
    "        hist_llr = (\n",
    "            -2\n",
    "            * sum_data_total_SR\n",
    "            * np.log((_sigma_asimov_SR(mu) / _sigma_asimov_SR(1.0)))\n",
    "        ) + (2 * (_sigma_asimov_SR(mu) - _sigma_asimov_SR(1.0)))\n",
    "\n",
    "        if use_CR:\n",
    "            hist_llr_CR = (\n",
    "                -2\n",
    "                * sum_data_total_CR\n",
    "                * np.log((_sigma_asimov_CR(mu) / _sigma_asimov_CR(1.0)))\n",
    "                ) + (2 * (_sigma_asimov_CR(mu) - _sigma_asimov_CR(1.0)))\n",
    "        else:\n",
    "            hist_llr_CR=0\n",
    "            #print(\"do not use CR\")    \n",
    "        #DR time is spent here ?        \n",
    "\n",
    "\n",
    "        comb_llr.append(hist_llr + hist_llr_CR)\n",
    "\n",
    "    comb_llr = np.array(comb_llr)\n",
    "    comb_llr = comb_llr - np.amin(comb_llr)\n",
    "\n",
    "    return comb_llr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train valid split\n",
    "\n",
    "X_train ,X_holdout, y_train, y_holdout, w_train, w_holdout = train_valid_split(data, labels, weights, valid_size=0.5)\n",
    "\n",
    "X_train ,X_valid, y_train, y_valid, w_train, w_valid = train_valid_split(X_train, y_train, w_train, valid_size=0.1)\n",
    "\n",
    "total_signal = np.sum(weights[labels == 1])\n",
    "total_signal_train = np.sum(w_train[y_train == 1])\n",
    "total_signal_valid = np.sum(w_valid[y_valid == 1])\n",
    "total_signal_holdout = np.sum(w_holdout[y_holdout == 1])\n",
    "w_train[y_train == 1] *= total_signal / total_signal_train\n",
    "w_valid[y_valid == 1] *= total_signal / total_signal_valid\n",
    "w_holdout[y_holdout == 1] *= total_signal / total_signal_holdout\n",
    "\n",
    "total_background = np.sum(weights[labels == 0])\n",
    "total_background_train = np.sum(w_train[y_train == 0])\n",
    "total_background_valid = np.sum(w_valid[y_valid == 0])\n",
    "total_background_holdout = np.sum(w_holdout[y_holdout == 0])\n",
    "w_train[y_train == 0] *= total_background / total_background_train\n",
    "w_valid[y_valid == 0] *= total_background / total_background_valid\n",
    "w_holdout[y_holdout == 0] *= total_background / total_background_holdout\n",
    "\n",
    "\n",
    "train_data = X_train.copy()\n",
    "train_data['labels'] = y_train\n",
    "train_data['weights'] = w_train\n",
    "\n",
    "train_data = postprocess(train_data)\n",
    "\n",
    "y_train = train_data.pop('labels')\n",
    "w_train = train_data.pop('weights')\n",
    "X_train = train_data.copy()\n",
    "\n",
    "del train_data\n",
    "\n",
    "holdout_data = X_holdout.copy()\n",
    "holdout_data['labels'] = y_holdout\n",
    "holdout_data['weights'] = w_holdout\n",
    "\n",
    "holdout_data = postprocess(holdout_data)\n",
    "\n",
    "y_holdout = holdout_data.pop('labels')\n",
    "w_holdout = holdout_data.pop('weights')\n",
    "X_holdout = holdout_data.copy()\n",
    "\n",
    "del holdout_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_weights_train = (w_train[y_train == 0].sum(), w_train[y_train == 1].sum())\n",
    "\n",
    "for i in range(len(class_weights_train)):  # loop on B then S target\n",
    "    # training dataset: equalize number of background and signal\n",
    "    w_train[y_train == i] *= max(class_weights_train) / class_weights_train[i]\n",
    "    # valid dataset : increase test weight to compensate for sampling\n",
    "\n",
    "print(\"[*] --- Training Model\")\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(n_estimators=100, n_jobs=4)\n",
    "model.fit(X_train_sc, y_train, sample_weight=w_train)\n",
    "\n",
    "y_pred_train = model.predict_proba(X_train_sc)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"[*] --- Training ROC AUC Score: \", roc_auc_score(y_train, y_pred_train[:, 1], sample_weight=w_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_min = 1\n",
    "theta_list = np.linspace(0.8, 1,100)\n",
    "for theta in theta_list:\n",
    "\n",
    "    print(\"theta: \", theta)\n",
    "\n",
    "    # predict probabilities for holdout\n",
    "    X_holdout_sc = scaler.transform(X_holdout)\n",
    "    y_pred = model.predict_proba(X_holdout_sc)\n",
    "    y_pred = y_pred[:,1]\n",
    "\n",
    "    y_pred = (y_pred>theta).astype(int)\n",
    "    gamma_roi = (w_holdout*(y_pred * y_holdout)).sum()\n",
    "    beta_roi = (w_holdout*(y_pred * (1-y_holdout))).sum()\n",
    "\n",
    "    valid_data = X_valid.copy()\n",
    "    valid_data['labels'] = y_valid\n",
    "    valid_data['weights'] = w_valid\n",
    "\n",
    "    valid_data_ps = postprocess(valid_data)\n",
    "\n",
    "    valid_label = valid_data_ps.pop('labels')\n",
    "    valid_weights = valid_data_ps.pop('weights')\n",
    "\n",
    "\n",
    "    X_valid_sc = scaler.fit_transform(valid_data_ps)\n",
    "\n",
    "    X_valid_sc = scaler.fit_transform(X_valid)\n",
    "\n",
    "    y_pred = model.predict_proba(X_valid_sc) \n",
    "\n",
    "    y_pred = y_pred[:,1]\n",
    "    y_pred = (y_pred>theta).astype(int)\n",
    "\n",
    "    weight = w_valid*(y_pred)\n",
    "\n",
    "    mu_scan = np.linspace(0, 3, 100)\n",
    "    hist_llr = calculate_NLL(mu_scan, weight, use_CR=False)\n",
    "    hist_llr = np.array(hist_llr)\n",
    "\n",
    "    val =  np.abs(mu_scan[np.argmin(hist_llr)] - 1)\n",
    "\n",
    "    if val < val_min:\n",
    "        val_min = val\n",
    "        print(\"val: \", val)\n",
    "        print(\"gamma_roi: \", gamma_roi)\n",
    "        print(\"beta_roi: \", beta_roi)\n",
    "        print(\"Uncertainity\", np.sqrt(gamma_roi + beta_roi)/gamma_roi)\n",
    "        Beta_roi = beta_roi\n",
    "        Gamma_roi = gamma_roi\n",
    "\n",
    "        best_theta = theta\n",
    "        hist_llr_best = hist_llr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_theta: \", best_theta)\n",
    "print(\"val_min: \", val_min)\n",
    "print(Beta_roi)\n",
    "print(Gamma_roi)\n",
    "del beta_roi\n",
    "del gamma_roi\n",
    "beta_roi = Beta_roi\n",
    "gamma_roi = Gamma_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"min NLL: \", min(hist_llr_best))\n",
    "print(\"min NLL index: \", np.argmin(hist_llr_best))\n",
    "print(\"min NLL mu: \", mu_scan[np.argmin(hist_llr_best)])\n",
    "\n",
    "\n",
    "sigma_7 = (max(mu_scan[np.where((hist_llr_best <= 49) & (hist_llr_best >= 36))]), min(mu_scan[np.where((hist_llr_best < 49) & (hist_llr_best > 36))]))\n",
    "sigma_6 = (max(mu_scan[np.where((hist_llr_best <= 36) & (hist_llr_best >= 25))]), min(mu_scan[np.where((hist_llr_best < 36) & (hist_llr_best > 25))]))\n",
    "sigma_5 = (max(mu_scan[np.where((hist_llr_best <= 25) & (hist_llr_best >= 16))]), min(mu_scan[np.where((hist_llr_best < 25) & (hist_llr_best > 16))]))\n",
    "sigma_4 = (max(mu_scan[np.where((hist_llr_best <= 16) & (hist_llr_best >= 9))]), min(mu_scan[np.where((hist_llr_best < 16) & (hist_llr_best > 9))]))\n",
    "sigma_3 = (max(mu_scan[np.where((hist_llr_best <= 9) & (hist_llr_best >= 4))]), min(mu_scan[np.where((hist_llr_best < 9) & (hist_llr_best > 4))]))\n",
    "sigma_2 = (max(mu_scan[np.where((hist_llr_best <= 4) & (hist_llr_best >= 1))]), min(mu_scan[np.where((hist_llr_best < 4) & (hist_llr_best > 1))]))\n",
    "sigma_1 = (max(mu_scan[np.where((hist_llr_best <= 1) & (hist_llr_best >= 0))]), min(mu_scan[np.where((hist_llr_best < 1) & (hist_llr_best > 0))]))\n",
    "\n",
    "\n",
    "plt.plot( mu_scan,hist_llr_best)\n",
    "plt.hlines(1, 0, 3,  colors=\"r\", linestyles=\"dashed\", label=f\"1 sigma: {sigma_1[0]:.3f} {sigma_1[1]:.3f}\")\n",
    "plt.hlines(4, 0, 3,  colors=\"b\", linestyles=\"dashed\", label=f\"2 sigma: {sigma_2[0]:.3f} {sigma_2[1]:.3f}\")\n",
    "plt.hlines(9, 0, 3,  colors=\"g\", linestyles=\"dashed\", label=f\"3 sigma: {sigma_3[0]:.3f} {sigma_3[1]:.3f}\")\n",
    "plt.hlines(16, 0, 3, colors=\"y\", linestyles=\"dashed\", label=f\"4 sigma: {sigma_4[0]:.3f} {sigma_4[1]:.3f}\")\n",
    "plt.hlines(25, 0, 3, colors=\"c\", linestyles=\"dashed\", label=f\"5 sigma: {sigma_5[0]:.3f} {sigma_5[1]:.3f}\")\n",
    "plt.hlines(36, 0, 3, colors=\"m\", linestyles=\"dashed\", label=f\"6 sigma: {sigma_6[0]:.3f} {sigma_6[1]:.3f}\")\n",
    "plt.hlines(49, 0, 3, colors=\"k\", linestyles=\"dashed\", label=f\"7 sigma: {sigma_7[0]:.3f} {sigma_7[1]:.3f}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"mu\")    \n",
    "plt.ylabel(\"NLL\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataframe(df):\n",
    "    # Convert DataFrame to NumPy array\n",
    "    array = df.to_numpy()\n",
    "\n",
    "    # Shuffle the array\n",
    "    np.random.shuffle(array)\n",
    "\n",
    "    # Convert the shuffled array back to a DataFrame\n",
    "    shuffled_df = pd.DataFrame(array, columns=df.columns)\n",
    "\n",
    "    return shuffled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p16s = []\n",
    "p84s = []\n",
    "test_data_file = os.path.join(input_dir, 'test', 'data', f'data.csv')\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "\n",
    "\n",
    "    signal_valid_bs = w_valid_bs[y_valid_bs == 1]\n",
    "    background_valid_bs = w_valid_bs[y_valid_bs == 0]\n",
    "    print(\"signal_valid_bs: \", signal_valid_bs.sum())\n",
    "    print(\"background_valid_bs: \", background_valid_bs.sum())\n",
    "\n",
    "\n",
    "\n",
    "    X_valid_bs = scaler.fit_transform(test_df)\n",
    "\n",
    "    y_pred = model.predict_proba(X_valid_bs)\n",
    "    y_pred = y_pred[:,1]\n",
    "\n",
    "    y_pred = (y_pred>best_theta).astype(int)\n",
    "\n",
    "\n",
    "    weight = w_valid_bs*(y_pred)\n",
    "\n",
    "    print(\"weight: \", weight.sum())\n",
    "    print(\"mu: \", (weight.sum() - beta_roi)/gamma_roi)\n",
    "    print(\"gamma_roi: \", gamma_roi)\n",
    "    print(\"beta_roi: \", beta_roi)\n",
    "\n",
    "\n",
    "    mu_scan = np.linspace(0, 3, 100)\n",
    "    hist_llr = calculate_NLL(mu_scan, weight, use_CR=False)\n",
    "    hist_llr = np.array(hist_llr)\n",
    "\n",
    "    if (mu_scan[np.where((hist_llr <= 1.0) & (hist_llr >= 0.0))].size == 0):\n",
    "        p16 = 0\n",
    "        p84 = 0\n",
    "    else:\n",
    "        p16 = min(mu_scan[np.where((hist_llr <= 1.0) & (hist_llr >= 0.0))])\n",
    "        p84 = max(mu_scan[np.where((hist_llr <= 1.0) & (hist_llr >= 0.0))]) \n",
    "\n",
    "    p16s.append(p16)\n",
    "    p84s.append(p84)\n",
    "\n",
    "    plt.plot( mu_scan,hist_llr)\n",
    "\n",
    "plt.hlines(1, 0, 3, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.hlines(4, 0, 3, colors=\"b\", linestyles=\"dashed\")\n",
    "plt.hlines(9, 0, 3, colors=\"y\", linestyles=\"dashed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (p16, p84) in enumerate(zip(p16s, p84s)):\n",
    "    plt.hlines(y=i, xmin=p16, xmax=p84, colors='b')\n",
    "\n",
    "\n",
    "\n",
    "print(\"p16s: \", p16s)\n",
    "print(\"p84s: \", p84s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantiles_Score( mu, p16, p84, eps=1e-3):\n",
    "    \n",
    "    def Interval(p16, p84):\n",
    "        \"\"\"Compute the average of the intervals defined by vectors p16 and p84.\"\"\"\n",
    "        return np.mean(p84 - p16)\n",
    "\n",
    "    def Coverage(mu, p16, p84):\n",
    "        \"\"\"Compute the fraction of times scalar mu is within intervals defined by vectors p16 and p84.\"\"\"\n",
    "        return_coverage = np.mean((mu >= p16) & (mu <= p84))\n",
    "        return return_coverage\n",
    "\n",
    "    def f(x, n_tries, max_coverage=1e4, one_sigma = 0.6827):\n",
    "            sigma68 = np.sqrt(((1-one_sigma)*one_sigma*n_tries))/n_tries\n",
    "\n",
    "            if (x >= one_sigma-2*sigma68 and x <= one_sigma+2*sigma68):\n",
    "                out = 1\n",
    "            elif (x < one_sigma-2*sigma68):\n",
    "                out = 1 + abs((x-(one_sigma-2*sigma68))/sigma68)**4\n",
    "            elif (x > one_sigma+2*sigma68):\n",
    "                out = 1 + abs((x-(one_sigma+2*sigma68))/sigma68)**3\n",
    "            return out\n",
    "\n",
    "\n",
    "    coverage = Coverage(mu, p16, p84)\n",
    "    interval = Interval(p16, p84)\n",
    "    score = -np.log((interval + eps) * f(coverage, n_tries=mu.shape[0]))\n",
    "    return interval, coverage, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = np.ones(100)\n",
    "p16s = np.array(p16s)\n",
    "p84s = np.array(p84s)\n",
    "\n",
    "print(\"Quantiles_Score: \", Quantiles_Score(mus, p16s, p84s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
